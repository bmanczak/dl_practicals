{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module implements various modules of the network.\n",
    "You should fill in code into indicated sections.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearModule(object):\n",
    "    \"\"\"\n",
    "    Linear module. Applies a linear transformation to the input data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Initializes the parameters of the module.\n",
    "\n",
    "        Args:\n",
    "          in_features: size of each input sample\n",
    "          out_features: size of each output sample\n",
    "\n",
    "        TODO:\n",
    "        Initialize weights self.params['weight'] using normal distribution with mean = 0 and\n",
    "        std = 0.0001. Initialize biases self.params['bias'] with 0.\n",
    "\n",
    "        Also, initialize gradients with zeros.\n",
    "        \"\"\"\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "\n",
    "        self.params['weight'] = np.random.normal(loc=0, scale=0.0001, size=(self.out_features, self.in_features))\n",
    "        # print(self.params['weight'].shape)\n",
    "        self.params['bias'] = np.zeros(shape=(1, self.out_features))\n",
    "        # self.params[\"dX\"] = np.zeros_like()\n",
    "        self.grads[\"weight\"] = np.zeros_like(self.params['weight'])\n",
    "        self.grads[\"bias\"] = np.zeros_like(self.params[\"bias\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "          x: input to the module\n",
    "        Returns:\n",
    "          out: output of the module\n",
    "\n",
    "        TODO:\n",
    "        Implement forward pass of the module.\n",
    "\n",
    "        Hint: You can store intermediate variables inside the object. They can be used in backward pass computation.\n",
    "        \"\"\"\n",
    "        self.params[\"X\"] = x\n",
    "\n",
    "        out = np.dot(self.params[\"X\"], self.params['weight'].T) + self.params[\"bias\"]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "\n",
    "        Args:\n",
    "          dout: gradients of the previous module\n",
    "        Returns:\n",
    "          dx: gradients with respect to the input of the module\n",
    "\n",
    "        TODO:\n",
    "        Implement backward pass of the module. Store gradient of the loss with respect to\n",
    "        layer parameters in self.grads['weight'] and self.grads['bias'].\n",
    "        \"\"\"\n",
    "\n",
    "        dx = np.dot(dout, self.params['weight'])\n",
    "        # self.grads[\"dX\"] = dx\n",
    "        self.grads[\"weight\"] = np.dot(dout.T, self.params[\"X\"])\n",
    "        self.grads[\"bias\"] = np.sum(dout, axis=0) / self.params[\"X\"].shape[0]\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.98234330e-08, -8.64501758e-08, -4.88522969e-08, ...,\n",
       "        -9.38219676e-09,  6.51560618e-08, -6.58250424e-08],\n",
       "       [ 1.54975503e-09, -1.94825694e-07, -1.91254849e-07, ...,\n",
       "        -1.09851598e-07,  1.18921198e-07,  9.52031706e-09],\n",
       "       [ 6.38657163e-09,  2.31610515e-07, -2.18490407e-08, ...,\n",
       "        -1.31588645e-08,  5.29574960e-09,  1.89888868e-08],\n",
       "       [-9.66480513e-08,  8.79447059e-09, -1.70072529e-07, ...,\n",
       "         1.61562282e-07, -3.23140152e-08, -1.32456720e-07]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_features = 100\n",
    "in_features = 3072\n",
    "batch_size = 4\n",
    "\n",
    "W = np.random.normal(loc = 0, scale = 0.0001, size = (out_features,in_features))\n",
    "b = np.zeros(shape = (1, out_features))\n",
    "\n",
    "X = np.random.normal(loc = 0, scale = 0.0001, size = (4,in_features))\n",
    "\n",
    "layer = LinearModule(in_features, out_features)\n",
    "layer.forward(X)\n",
    "dout = np.random.normal(loc = 0, scale = 0.0001, size = (batch_size,out_features))\n",
    "layer.backward(dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxModule(object):\n",
    "    \"\"\"\n",
    "    Softmax activation module.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "          x: input to the module\n",
    "        Returns:\n",
    "          out: output of the module\n",
    "\n",
    "        TODO:\n",
    "        Implement forward pass of the module.\n",
    "        To stabilize computation you should use the so-called Max Trick - https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "\n",
    "        Hint: You can store intermediate variables inside the object. They can be used in backward pass computation.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "\n",
    "        X_max = np.max(x, axis=1)\n",
    "        numerator = np.exp(x - X_max.reshape(X_max.shape[0], 1))\n",
    "        denominator = np.sum(numerator, axis=1)\n",
    "        out = numerator / denominator.reshape(denominator.shape[0], 1)\n",
    "        self.params[\"Y\"] = out\n",
    "        #print(\"Logits sum to:\", np.sum(out, axis = 0))\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        Args:\n",
    "          dout: gradients of the previous modul\n",
    "        Returns:\n",
    "          dx: gradients with respect to the input of the module\n",
    "\n",
    "        TODO:\n",
    "        Implement backward pass of the module.\n",
    "        \"\"\"\n",
    "\n",
    "        dx = np.multiply(dout, self.params[\"Y\"]) - np.einsum(\"in,in,ij->ij\", dout, self.params[\"Y\"], self.params[\"Y\"])\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3,8],\n",
    "             [4,5,6,7]])\n",
    "x_max = x.max() #np.max(x)#.max()\n",
    "numerator = np.exp(x - x_max)\n",
    "out = numerator/numerator.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 10 \n",
    "X = np.random.normal(loc = 0, scale = 0.0001, size = (batch_size ,num_classes))\n",
    "dout = np.random.normal(loc = 0, scale = 0.0001, size = (batch_size ,num_classes))\n",
    "\n",
    "soft_module = SoftMaxModule()\n",
    "soft_module.forward(X)\n",
    "\n",
    "soft_module.backward(dout).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39999675, 0.39999113, 0.40001994, 0.39999558, 0.40001081,\n",
       "       0.40002469, 0.39999399, 0.39999956, 0.39996543, 0.40000212])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(soft_module.forward(X), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyModule(object):\n",
    "    \"\"\"\n",
    "    Cross entropy loss module.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "          x: input to the module\n",
    "          y: labels of the input\n",
    "        Returns:\n",
    "          out: cross entropy loss\n",
    "    \n",
    "        TODO:\n",
    "        Implement forward pass of the module.\n",
    "        \"\"\"\n",
    "        \n",
    "        out = -(1/x.shape[0])*np.sum(np.multiply(np.log(x), y))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, x, y):\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        Args:\n",
    "          x: input to the module\n",
    "          y: labels of the input\n",
    "        Returns:\n",
    "          dx: gradient of the loss with the respect to the input x.\n",
    "    \n",
    "        TODO:\n",
    "        Implement backward pass of the module.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.grads = {}\n",
    "        \n",
    "        dx = -(1/x.shape[0]) * (y/x)\n",
    "        self.grads[\"dX\"] = dx\n",
    "        return dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifar10_utils\n",
    "cifar10 = cifar10_utils.get_cifar10(\"cifar10/cifar-10-batches-py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = cifar10[\"train\"].next_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.normal(loc = 1, scale = 0.0001, size = (batch_size ,num_classes)) # predictions \n",
    "#-1/batch_size * y/X\n",
    "cross_entropy_module = CrossEntropyModule()\n",
    "cross_entropy_module.forward(X,y)\n",
    "cross_entropy_module.backward(X,y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELUModule(object):\n",
    "    \"\"\"\n",
    "    ELU activation module.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "          x: input to the module\n",
    "        Returns:\n",
    "          out: output of the module\n",
    "\n",
    "        TODO:\n",
    "        Implement forward pass of the module.\n",
    "\n",
    "        Hint: You can store intermediate variables inside the object. They can be used in backward pass computation.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        \n",
    "        \n",
    "        out = np.where(x>0, x, np.exp(x)-1)\n",
    "        self.params[\"X\"] = x\n",
    "        #self.params[\"Y\"] = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        Args:\n",
    "          dout: gradients of the previous module\n",
    "        Returns:\n",
    "          dx: gradients with respect to the input of the module\n",
    "\n",
    "        TODO:\n",
    "        Implement backward pass of the module.\n",
    "        \"\"\"\n",
    "        \n",
    "        dx = np.multiply(dout, np.where(self.params[\"X\"] > 0, 1, np.exp(self.params[\"X\"]) ))\n",
    "        return dx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0670965 , -0.36644172,  0.9051607 ,  1.67229858, -0.25565506,\n",
       "         1.09021647,  0.20771274,  2.54392489,  0.53670011,  1.7362274 ,\n",
       "         2.88015647,  0.23585116,  1.09048265,  1.09476885,  2.02692423,\n",
       "         1.81532632,  1.20029214,  1.85869154,  2.49524234,  1.82180612,\n",
       "         1.37553863,  1.15386784,  2.93909276,  0.40615689, -0.35935099,\n",
       "        -0.04527735, -0.19741476,  0.54605668,  0.77041633, -0.15663866,\n",
       "         3.15102593,  1.46579437,  1.93769673,  0.30645676,  1.0466298 ,\n",
       "        -0.29833038,  0.92046318, -0.01867387, -0.25336328,  1.84005212,\n",
       "         0.21469715,  1.42993348,  1.04658296,  0.2631546 ,  0.33055544,\n",
       "         0.35086653,  1.53659629, -0.36326589,  1.03664124, -0.35062794,\n",
       "         2.32175707, -0.33362329, -0.0651303 , -0.01929105,  1.64988477,\n",
       "         0.91026567,  1.39360793,  2.39782841,  0.31184157,  1.4822206 ,\n",
       "         1.46169907,  1.07378392, -0.14987814,  1.99072383,  2.91433749,\n",
       "         0.69835592,  1.60215791, -0.35262179,  0.14291238,  0.33716396,\n",
       "         0.92818603, -0.03983126,  1.30131064,  0.18641048,  0.36110777,\n",
       "         2.11156472,  0.63339425,  1.77664367,  1.5328074 ,  2.64926481,\n",
       "         3.51529109, -0.36566804,  0.07503041,  1.00284165,  0.43674479,\n",
       "         1.01461237,  1.04857871,  0.17478046, -0.2449684 ,  2.21972269,\n",
       "         2.12984741,  0.17378766, -0.30156959,  1.20419107, -0.36777684,\n",
       "         0.58927948,  1.59456183,  2.70455227,  1.25805474,  2.39471262],\n",
       "       [ 0.90541588,  1.76953787, -0.13744614,  1.32326141,  2.22619772,\n",
       "         1.65031976,  1.25575609, -0.35421604, -0.36281962,  1.73682917,\n",
       "         1.47216373,  1.09295591,  2.09192127,  0.60026717,  0.65262046,\n",
       "         1.32740199,  1.71827567,  0.81033186, -0.16371587,  1.53029716,\n",
       "         1.57894827,  1.54614981,  2.40608495, -0.36559517,  0.62594332,\n",
       "         2.66842663,  2.19918881,  0.74131411,  0.45950958,  1.16241264,\n",
       "        -0.24855977, -0.32075172,  0.06794797,  0.3888509 ,  0.90643976,\n",
       "         1.83120134,  2.48715654,  1.79332537,  1.56774916,  2.16604597,\n",
       "         1.68101598,  0.65713442,  1.93114342,  1.40805881,  1.88347788,\n",
       "         0.76749423, -0.26556478,  2.13531536,  1.95536786,  0.60111844,\n",
       "         1.18468726,  1.30710715,  1.79565415,  1.27745073, -0.30005563,\n",
       "        -0.19792587,  2.95907209,  2.26645929,  3.16891807,  0.09683072,\n",
       "         1.89523927,  1.97132187, -0.27060292, -0.06509062,  1.57135256,\n",
       "        -0.318884  ,  0.44498201,  0.58795228, -0.36045257,  0.23079321,\n",
       "         1.28812304,  0.98930073,  1.59903612,  1.38540869,  0.88780213,\n",
       "         3.46472794,  1.03299879,  1.28577424,  0.9824193 ,  0.74356727,\n",
       "        -0.30315204, -0.04719789, -0.32850229,  0.77281999,  0.61429606,\n",
       "        -0.34113403,  1.79775787,  1.94706091, -0.3597275 ,  0.59467663,\n",
       "         2.76890459,  0.86655642, -0.30659764,  0.53318413,  0.96738573,\n",
       "         2.76937521,  0.12425785,  0.0100123 , -0.34314435,  1.01003066],\n",
       "       [ 1.54857465,  2.27970908,  2.99863109,  2.33019894,  3.39421569,\n",
       "         0.35801314,  3.35242026,  1.68815662,  0.40427859, -0.00669495,\n",
       "         0.97595608,  0.69281571,  2.30477999,  1.65353556, -0.04222244,\n",
       "         2.31120809, -0.35284128,  1.0748463 ,  0.71132151,  2.07786686,\n",
       "         0.8578686 , -0.04341097,  0.96492014,  2.15424328,  0.31066146,\n",
       "         2.74072267,  1.52891693,  0.99745669,  1.13415926,  0.92820009,\n",
       "         2.26877379, -0.20576447,  0.4365778 ,  0.81135128,  1.8223881 ,\n",
       "         0.63577859,  2.63713314,  1.24363147,  0.6823428 ,  0.80080166,\n",
       "         1.99758192,  1.93514325,  0.91971645,  2.37400323,  1.37291098,\n",
       "         1.02171084,  0.85074305,  0.91273012,  0.34858143,  1.74773576,\n",
       "         1.02984309, -0.11249441,  0.55819725,  2.69730918,  1.86633834,\n",
       "         0.87803562,  1.96975675,  1.51458892,  0.2520589 ,  1.51469723,\n",
       "         1.06776909,  1.92594381,  1.51174518,  0.97843337,  0.85777545,\n",
       "         1.9485642 ,  0.38100236, -0.31245629,  1.11641315, -0.22909194,\n",
       "        -0.03774307,  1.38896936,  2.13210267,  1.46525883, -0.08939402,\n",
       "        -0.28533756,  0.80138258,  1.41012877,  1.52687289,  0.90384371,\n",
       "         0.10211877,  0.66472951,  0.94265588,  1.21779781,  1.86417181,\n",
       "         1.21649282,  1.77024616,  1.47107709,  0.53868891,  2.01117171,\n",
       "        -0.28318916, -0.01102515,  1.18313531,  2.53128864,  0.96962494,\n",
       "        -0.34191233, -0.35661866,  1.05609508,  2.16728822,  1.23909075],\n",
       "       [ 1.19405168,  0.52968361,  2.08329837,  0.37809593,  1.50664397,\n",
       "         1.40923692,  1.97234796,  0.66451074,  0.60325843,  0.7632684 ,\n",
       "        -0.35005464,  0.71431838,  1.34116559,  2.27232312,  1.72680253,\n",
       "         1.63413168,  1.93295654,  0.77534491,  0.18356379,  1.79417182,\n",
       "         0.84842921,  1.59609555,  0.59284909,  1.7601775 , -0.07540717,\n",
       "         0.9904409 ,  1.83549009,  0.65383504, -0.04918274,  0.74688773,\n",
       "         0.68187037, -0.32112265,  1.61011796,  0.94302317,  0.84675822,\n",
       "         2.36145458,  2.26845334,  1.6625025 , -0.20014167,  0.73827174,\n",
       "         0.30940791,  1.46593134,  1.98329475,  1.6218579 ,  1.29787101,\n",
       "        -0.3475817 ,  0.55999797,  2.82015674,  1.93454   , -0.32342706,\n",
       "        -0.18584238,  1.28522806, -0.14073358,  1.07029022,  0.62317668,\n",
       "         2.10830002,  0.24923717, -0.36238546,  1.65833597,  1.80463908,\n",
       "         0.71338807,  1.72307514,  1.6081152 ,  0.34265272,  1.96394694,\n",
       "         1.43673964,  1.18334959,  2.91030051,  0.84970238,  0.92658171,\n",
       "         0.5850263 ,  1.05695463,  0.78443033, -0.36680008,  0.90722706,\n",
       "         0.70935047,  0.98155147,  1.90657247,  1.0005468 ,  2.5877467 ,\n",
       "         0.1488355 ,  0.5919934 ,  0.97612986,  0.29214176,  0.96843039,\n",
       "         0.6112174 ,  1.42776506,  2.0189912 ,  1.80374171,  0.1554273 ,\n",
       "         1.09103263,  1.45421894,  0.50802072,  2.2724461 , -0.18713954,\n",
       "         2.18552604,  1.45503127,  2.16101232,  0.36411157,  1.27306588]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.normal(loc = 1, scale = 1, size = (batch_size ,out_features)) # predictions \n",
    "#dout = \n",
    "\n",
    "elu_module = ELUModule()\n",
    "elu_module.forward(X)\n",
    "elu_module.backward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual example of forward and backproagation\n",
    "For one layer neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = cifar10[\"train\"].next_batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_module1 = LinearModule(in_features = 3072, out_features=100)\n",
    "a1 = linear_module1.forward(x.reshape(x.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<modules.LinearModule at 0x7fd6009877c0>"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_module1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "elu_module1 = ELUModule()\n",
    "h1 = elu_module1.forward(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_module2 = LinearModule(in_features = 100, out_features=10)\n",
    "a2 = linear_module2.forward(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "elu_module2 = ELUModule()\n",
    "h2 = elu_module2.forward(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_module = SoftMaxModule()\n",
    "s = softmax_module.forward(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_module = CrossEntropyModule()\n",
    "loss = cross_entropy_module.forward(s, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL = cross_entropy_module.backward(s,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh2 = elu_module2.backward(dL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "da2 = linear_module2.backward(dh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1922161910457639e-06\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(linear_module2.params[\"weight\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_module2.params[\"weight\"] -= learning_rate*linear_module2.grads[\"weight\"]\n",
    "linear_module2.params[\"bias\"] -= learning_rate*linear_module2.grads[\"bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6124889748199153e-05\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(linear_module2.params[\"weight\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh1 = elu_module1.backward(da2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "da1 = linear_module1.backward(dh1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_module1.params[\"weight\"] -= learning_rate*linear_module1.grads[\"weight\"]\n",
    "linear_module1.params[\"bias\"] -= learning_rate*linear_module1.grads[\"bias\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MLP object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module implements a multi-layer perceptron (MLP) in NumPy.\n",
    "You should fill in code into indicated sections.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from modules import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"\n",
    "    This class implements a Multi-layer Perceptron in NumPy.\n",
    "    It handles the different layers and parameters of the model.\n",
    "    Once initialized an MLP object can perform forward and backward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden, n_classes):\n",
    "        \"\"\"\n",
    "        Initializes MLP object.\n",
    "\n",
    "        Args:\n",
    "          n_inputs: number of inputs.\n",
    "          n_hidden: list of ints, specifies the number of units\n",
    "                    in each linear layer. If the list is empty, the MLP\n",
    "                    will not have any linear layers, and the model\n",
    "                    will simply perform a multinomial logistic regression.\n",
    "          n_classes: number of classes of the classification problem.\n",
    "                     This number is required in order to specify the\n",
    "                     output dimensions of the MLP\n",
    "\n",
    "        TODO:\n",
    "        Implement initialization of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "         \n",
    "        self.modules = {}\n",
    "        \n",
    "        sizes = [n_inputs] + n_hidden + [n_classes]\n",
    "        \n",
    "        \n",
    "        for layer_id, size in enumerate(sizes[:-1]):\n",
    "            \n",
    "            self.modules[\"linear_\" + str(layer_id) ] = LinearModule(\n",
    "                                                                in_features = size,\n",
    "                                                                 out_features = sizes[layer_id + 1])\n",
    "            self.modules[\"act_\" + str(layer_id)] = ELUModule()\n",
    "            \n",
    "        \n",
    "        self.modules[\"softmax\"] = SoftMaxModule()\n",
    "        #self.modules[\"loss\"] = CrossEntropyModule()\n",
    "            \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass of the input. Here an input tensor x is transformed through\n",
    "        several layer transformations.\n",
    "\n",
    "        Args:\n",
    "          x: input to the network\n",
    "        Returns:\n",
    "          out: outputs of the network\n",
    "\n",
    "        TODO:\n",
    "        Implement forward pass of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        out = x\n",
    "        for name, module in self.modules.items():\n",
    "            #print(name)\n",
    "            out = self.modules[name].forward(out)\n",
    "            #if \"linear\" in name:\n",
    "             #   print(\"Mean of the used weights:\", np.mean(self.modules[name].params[\"weight\"] ))\n",
    "            #print(out.shape)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Performs backward pass given the gradients of the loss.\n",
    "\n",
    "        Args:\n",
    "          dout: gradients of the loss\n",
    "\n",
    "        TODO:\n",
    "        Implement backward pass of the network.\n",
    "        \"\"\"\n",
    "        \n",
    "        #self.grads = {} \n",
    "        for name, module in reversed(self.modules.items()):\n",
    "            #print(name)\n",
    "            dout = self.modules[name].backward(dout)\n",
    "            #if \"linear\" in name:\n",
    "                #print(np.mean())\n",
    "        \n",
    "\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mlp = MLP(3072, [100], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_0\n",
      "(4, 100)\n",
      "act_0\n",
      "(4, 100)\n",
      "linear_1\n",
      "(4, 10)\n",
      "act_1\n",
      "(4, 10)\n",
      "softmax\n",
      "(4, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(my_mlp.forward(x.reshape(x.shape[0], -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear_0': <modules.LinearModule at 0x7fd3b1c87070>,\n",
       " 'act_0': <modules.ELUModule at 0x7fd3b1d301c0>,\n",
       " 'linear_1': <modules.LinearModule at 0x7fd3b1c876a0>,\n",
       " 'act_1': <modules.ELUModule at 0x7fd3b1c87a30>,\n",
       " 'softmax': <modules.SoftMaxModule at 0x7fd3b1c87550>}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0d5c84af4b62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "my_mlp.forward(x.reshape(x.shape[0], -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3072)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(x.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax\n",
      "act_1\n",
      "linear_1\n",
      "act_0\n",
      "linear_0\n"
     ]
    }
   ],
   "source": [
    "my_mlp.backward(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear_0': <modules.LinearModule at 0x7fd600789ee0>,\n",
       " 'act_0': <modules.ELUModule at 0x7fd60281a9d0>,\n",
       " 'linear_1': <modules.LinearModule at 0x7fd602343910>,\n",
       " 'act_1': <modules.ELUModule at 0x7fd601413790>,\n",
       " 'softmax': <modules.SoftMaxModule at 0x7fd600789400>}"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.modules[\"linear_1\"].grads['weight'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default constants\n",
    "DNN_HIDDEN_UNITS_DEFAULT = '100'\n",
    "LEARNING_RATE_DEFAULT = 1e-2\n",
    "MAX_STEPS_DEFAULT = 4000 #1400\n",
    "BATCH_SIZE_DEFAULT = 200\n",
    "EVAL_FREQ_DEFAULT = 100\n",
    "dnn_hidden_units = [100, 50]\n",
    "# Directory in which cifar data is saved\n",
    "DATA_DIR_DEFAULT = './cifar10/cifar-10-batches-py'\n",
    "\n",
    "FLAGS = None\n",
    "import cifar10_utils\n",
    "\n",
    "def accuracy(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy, i.e. the average of correct predictions\n",
    "    of the network.\n",
    "\n",
    "    Args:\n",
    "      predictions: 2D float array of size [batch_size, n_classes]\n",
    "      labels: 2D int array of size [batch_size, n_classes]\n",
    "              with one-hot encoding. Ground truth labels for\n",
    "              each sample in the batch\n",
    "    Returns:\n",
    "      accuracy: scalar float, the accuracy of predictions,\n",
    "                i.e. the average correct predictions over the whole batch\n",
    "\n",
    "    TODO:\n",
    "    Implement accuracy computation.\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = ( predictions.argmax(axis = 1) == targets.argmax(axis = 1)).sum()/predictions.shape[0]\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def test_model(net, dataset, loss_module, batch_size = BATCH_SIZE_DEFAULT):\n",
    "    \"\"\"\n",
    "    Test a model for accuracy and loss on a specified dataset. Extended upon the tutorial on Activation Functions.\n",
    "\n",
    "    Inputs:\n",
    "        net -MLP object\n",
    "        dataset - dataset object (train or test)\n",
    "        loss_module - module used to calculate the loss\n",
    "        batch_size - size of fetchted batches\n",
    "    \"\"\"\n",
    "    iters_needed = dataset._num_examples//batch_size # iterations needed to check the whole dataset\n",
    "    #print(iters_needed)\n",
    "    loss = 0\n",
    "    true_preds = 0\n",
    "    count = 0\n",
    "    #print(batch_size)\n",
    "    for i in range(iters_needed):\n",
    "        imgs,labels = dataset.next_batch(batch_size)\n",
    "        imgs = imgs.reshape(imgs.shape[0], -1)\n",
    "        preds = net.forward(imgs)\n",
    "        true_preds += (preds.argmax(axis = 1) == labels.argmax(axis = 1)).sum()\n",
    "        count += labels.shape[0]\n",
    "        loss += loss_module.forward(preds, labels)\n",
    "    #print(count)\n",
    "    acc = true_preds/count\n",
    "    loss /= count\n",
    "    return acc, loss \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Performs training and evaluation of MLP model.\n",
    "\n",
    "    TODO:\n",
    "    Implement training and evaluation of MLP model. Evaluate your model on the whole test set each eval_freq iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    ### DO NOT CHANGE SEEDS!\n",
    "    # Set the random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Prepare all functions\n",
    "    # Get number of units in each hidden layer specified in the string such as 100,100\n",
    "    if FLAGS.dnn_hidden_units:\n",
    "        dnn_hidden_units = FLAGS.dnn_hidden_units.split(\",\")\n",
    "        dnn_hidden_units = [int(dnn_hidden_unit_) for dnn_hidden_unit_ in dnn_hidden_units]\n",
    "    else:\n",
    "        dnn_hidden_units = []\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the network and loss module\n",
    "    the_mlp = MLP(3072, dnn_hidden_units, 10)\n",
    "    loss_module = CrossEntropyModule()\n",
    "    \n",
    "    # Load the data generator\n",
    "    cifar10 = cifar10_utils.get_cifar10(DATA_DIR_DEFAULT)\n",
    "    step = 0 # counter for steps\n",
    "    \n",
    "    #Train/test split\n",
    "    trainset = cifar10['train']\n",
    "    testset = cifar10['test']\n",
    "    accuracies_test = []\n",
    "    accuracies_train = []\n",
    "    losses_train = []\n",
    "    losses_test =[]\n",
    "    \n",
    "    while step<= MAX_STEPS_DEFAULT:\n",
    "        imgs, labels = trainset.next_batch(BATCH_SIZE_DEFAULT)\n",
    "        imgs = imgs.reshape(imgs.shape[0], -1)\n",
    "        preds = the_mlp.forward(imgs)\n",
    "        \n",
    "        #print(\"Mean preds\",np.mean(preds))\n",
    "        #print(\"First row of preds\", preds[0])\n",
    "        #print(preds.shape)\n",
    "        loss = loss_module.forward(preds, labels)\n",
    "        \n",
    "        # Backward step\n",
    "        dL = loss_module.backward(preds, labels)\n",
    "        the_mlp.backward(dL)\n",
    "        \n",
    "        \n",
    "        # Update parameters for each trainable layer\n",
    "        for layer_name, module in the_mlp.modules.items():\n",
    "            if \"linear\" in layer_name: # traianable layers in our case\n",
    "                #print(\"updating\")\n",
    "                #print('mean gradient:',np.mean(the_mlp.modules[layer_name].grads[\"weight\"]))\n",
    "                #print(\"mean weight\", np.mean(the_mlp.modules[layer_name].params[\"weight\"]))\n",
    "                #the_mlp.modules[name].params[\"weight\"] -= np.ones(shape = the_mlp.modules[name].params[\"weight\"].shape )\n",
    "                the_mlp.modules[layer_name].params[\"weight\"] -= LEARNING_RATE_DEFAULT*the_mlp.modules[layer_name].grads[\"weight\"]\n",
    "                the_mlp.modules[layer_name].params[\"bias\"] -= LEARNING_RATE_DEFAULT*the_mlp.modules[layer_name].grads[\"bias\"]\n",
    "        \n",
    "        #if step % EVAL_FREQ_DEFAULT == EVAL_FREQ_DEFAULT - 1:  #\n",
    "         #   print(\"[INFO]: Evaluation at step {} ...\".format(step))\n",
    "        #step += 1\n",
    "        \n",
    "        \n",
    "        if step % EVAL_FREQ_DEFAULT == EVAL_FREQ_DEFAULT - 1:  #\n",
    "            print(\"[INFO]: Evaluation at step {} ...\".format(step))\n",
    "            acc_at_step_test, loss_at_step_test = test_model(the_mlp, testset, loss_module,BATCH_SIZE_DEFAULT)\n",
    "            acc_at_step_train, loss_at_step_train = test_model(the_mlp, trainset, loss_module,BATCH_SIZE_DEFAULT)\n",
    "            accuracies_test.append(acc_at_step_test)\n",
    "            accuracies_train.append(acc_at_step_train)\n",
    "            losses_test.append(loss_at_step_test)\n",
    "            losses_train.append(loss_at_step_train)\n",
    "            print(\"[INFO]: Train acc {}%, Test acc {}%\".format(round(acc_at_step_train*100,4), round(acc_at_step_test*100,4)  ))\n",
    "            print(\"[INFO]: Train loss {}, Test loss {}\".format(round(loss_at_step_train*100,2), round(loss_at_step_test*100,2)  ))\n",
    "            \n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    return the_mlp    \n",
    "    return accuracies_test,accuracies_train, losses_train, losses_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_mlp = MLP(3072, [20, 10], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Evaluation at step 99 ...\n",
      "[INFO]: Train acc 9.938%, Test acc 10.0%\n",
      "[INFO]: Train loss 1.15, Test loss 1.15\n",
      "[INFO]: Evaluation at step 199 ...\n",
      "[INFO]: Train acc 10.09%, Test acc 10.0%\n",
      "[INFO]: Train loss 1.15, Test loss 1.15\n",
      "[INFO]: Evaluation at step 299 ...\n",
      "[INFO]: Train acc 9.938%, Test acc 10.0%\n",
      "[INFO]: Train loss 1.15, Test loss 1.15\n",
      "[INFO]: Evaluation at step 399 ...\n",
      "[INFO]: Train acc 9.68%, Test acc 10.0%\n",
      "[INFO]: Train loss 1.15, Test loss 1.15\n",
      "[INFO]: Evaluation at step 499 ...\n",
      "[INFO]: Train acc 10.0%, Test acc 10.0%\n",
      "[INFO]: Train loss 1.15, Test loss 1.15\n",
      "[INFO]: Evaluation at step 599 ...\n",
      "[INFO]: Train acc 9.834%, Test acc 10.0%\n",
      "[INFO]: Train loss 1.15, Test loss 1.15\n",
      "[INFO]: Evaluation at step 699 ...\n",
      "[INFO]: Train acc 18.164%, Test acc 17.94%\n",
      "[INFO]: Train loss 1.05, Test loss 1.05\n",
      "[INFO]: Evaluation at step 799 ...\n",
      "[INFO]: Train acc 19.406%, Test acc 18.95%\n",
      "[INFO]: Train loss 1.02, Test loss 1.01\n",
      "[INFO]: Evaluation at step 899 ...\n",
      "[INFO]: Train acc 25.116%, Test acc 25.23%\n",
      "[INFO]: Train loss 0.96, Test loss 0.96\n",
      "[INFO]: Evaluation at step 999 ...\n",
      "[INFO]: Train acc 34.244%, Test acc 34.35%\n",
      "[INFO]: Train loss 0.9, Test loss 0.9\n",
      "[INFO]: Evaluation at step 1099 ...\n",
      "[INFO]: Train acc 37.932%, Test acc 38.45%\n",
      "[INFO]: Train loss 0.85, Test loss 0.85\n",
      "[INFO]: Evaluation at step 1199 ...\n",
      "[INFO]: Train acc 40.544%, Test acc 40.51%\n",
      "[INFO]: Train loss 0.82, Test loss 0.83\n",
      "[INFO]: Evaluation at step 1299 ...\n",
      "[INFO]: Train acc 42.86%, Test acc 42.13%\n",
      "[INFO]: Train loss 0.8, Test loss 0.8\n",
      "[INFO]: Evaluation at step 1399 ...\n",
      "[INFO]: Train acc 43.248%, Test acc 43.11%\n",
      "[INFO]: Train loss 0.79, Test loss 0.8\n",
      "[INFO]: Evaluation at step 1499 ...\n",
      "[INFO]: Train acc 45.982%, Test acc 44.17%\n",
      "[INFO]: Train loss 0.76, Test loss 0.78\n",
      "[INFO]: Evaluation at step 1599 ...\n",
      "[INFO]: Train acc 47.752%, Test acc 46.71%\n",
      "[INFO]: Train loss 0.74, Test loss 0.75\n",
      "[INFO]: Evaluation at step 1699 ...\n",
      "[INFO]: Train acc 48.744%, Test acc 46.87%\n",
      "[INFO]: Train loss 0.72, Test loss 0.75\n",
      "[INFO]: Evaluation at step 1799 ...\n",
      "[INFO]: Train acc 47.296%, Test acc 46.26%\n",
      "[INFO]: Train loss 0.73, Test loss 0.76\n",
      "[INFO]: Evaluation at step 1899 ...\n",
      "[INFO]: Train acc 50.372%, Test acc 47.67%\n",
      "[INFO]: Train loss 0.7, Test loss 0.74\n",
      "[INFO]: Evaluation at step 1999 ...\n",
      "[INFO]: Train acc 50.786%, Test acc 47.18%\n",
      "[INFO]: Train loss 0.7, Test loss 0.75\n",
      "[INFO]: Evaluation at step 2099 ...\n",
      "[INFO]: Train acc 52.064%, Test acc 48.25%\n",
      "[INFO]: Train loss 0.68, Test loss 0.73\n",
      "[INFO]: Evaluation at step 2199 ...\n",
      "[INFO]: Train acc 51.868%, Test acc 47.58%\n",
      "[INFO]: Train loss 0.68, Test loss 0.74\n",
      "[INFO]: Evaluation at step 2299 ...\n",
      "[INFO]: Train acc 52.64%, Test acc 48.79%\n",
      "[INFO]: Train loss 0.67, Test loss 0.73\n",
      "[INFO]: Evaluation at step 2399 ...\n",
      "[INFO]: Train acc 54.102%, Test acc 49.13%\n",
      "[INFO]: Train loss 0.66, Test loss 0.72\n",
      "[INFO]: Evaluation at step 2499 ...\n",
      "[INFO]: Train acc 54.544%, Test acc 49.16%\n",
      "[INFO]: Train loss 0.64, Test loss 0.72\n",
      "[INFO]: Evaluation at step 2599 ...\n",
      "[INFO]: Train acc 54.742%, Test acc 49.46%\n",
      "[INFO]: Train loss 0.64, Test loss 0.72\n",
      "[INFO]: Evaluation at step 2699 ...\n",
      "[INFO]: Train acc 55.466%, Test acc 49.66%\n",
      "[INFO]: Train loss 0.63, Test loss 0.72\n",
      "[INFO]: Evaluation at step 2799 ...\n",
      "[INFO]: Train acc 56.546%, Test acc 50.55%\n",
      "[INFO]: Train loss 0.62, Test loss 0.7\n",
      "[INFO]: Evaluation at step 2899 ...\n",
      "[INFO]: Train acc 56.11%, Test acc 49.84%\n",
      "[INFO]: Train loss 0.62, Test loss 0.71\n",
      "[INFO]: Evaluation at step 2999 ...\n",
      "[INFO]: Train acc 58.414%, Test acc 50.57%\n",
      "[INFO]: Train loss 0.59, Test loss 0.7\n",
      "[INFO]: Evaluation at step 3099 ...\n",
      "[INFO]: Train acc 55.648%, Test acc 49.39%\n",
      "[INFO]: Train loss 0.63, Test loss 0.73\n",
      "[INFO]: Evaluation at step 3199 ...\n",
      "[INFO]: Train acc 58.442%, Test acc 50.92%\n",
      "[INFO]: Train loss 0.59, Test loss 0.71\n",
      "[INFO]: Evaluation at step 3299 ...\n",
      "[INFO]: Train acc 58.334%, Test acc 51.01%\n",
      "[INFO]: Train loss 0.59, Test loss 0.7\n",
      "[INFO]: Evaluation at step 3399 ...\n",
      "[INFO]: Train acc 59.48%, Test acc 50.96%\n",
      "[INFO]: Train loss 0.58, Test loss 0.7\n",
      "[INFO]: Evaluation at step 3499 ...\n",
      "[INFO]: Train acc 59.242%, Test acc 50.87%\n",
      "[INFO]: Train loss 0.58, Test loss 0.71\n",
      "[INFO]: Evaluation at step 3599 ...\n",
      "[INFO]: Train acc 59.268%, Test acc 50.95%\n",
      "[INFO]: Train loss 0.58, Test loss 0.72\n",
      "[INFO]: Evaluation at step 3699 ...\n",
      "[INFO]: Train acc 59.622%, Test acc 50.94%\n",
      "[INFO]: Train loss 0.57, Test loss 0.71\n",
      "[INFO]: Evaluation at step 3799 ...\n",
      "[INFO]: Train acc 59.642%, Test acc 50.87%\n",
      "[INFO]: Train loss 0.57, Test loss 0.72\n",
      "[INFO]: Evaluation at step 3899 ...\n",
      "[INFO]: Train acc 59.734%, Test acc 51.26%\n",
      "[INFO]: Train loss 0.57, Test loss 0.73\n",
      "[INFO]: Evaluation at step 3999 ...\n",
      "[INFO]: Train acc 60.852%, Test acc 50.37%\n",
      "[INFO]: Train loss 0.55, Test loss 0.72\n"
     ]
    }
   ],
   "source": [
    "vals  = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network and loss module\n",
    "#the_mlp = MLP(3072, dnn_hidden_units, 10)\n",
    "loss_module = CrossEntropyModule()\n",
    "\n",
    "# Load the data generator\n",
    "cifar10 = cifar10_utils.get_cifar10(DATA_DIR_DEFAULT)\n",
    "step = 0 # counter for steps\n",
    "\n",
    "#Train/test split\n",
    "trainset = cifar10['train']\n",
    "testset = cifar10['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4874, 0.007277671073820649)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(vals, testset,loss_module, 200 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = testset.next_batch(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4874"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((vals.forward(x.reshape(x.shape[0], -1)).argmax(axis= 1)) == (y.argmax(axis = 1))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4874"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(vals.forward(x.reshape(x.shape[0], -1)), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3,\n",
       "       6, 6, 2, 6, 3, 5, 4, 0, 0, 9, 1, 3, 4, 0, 3, 7, 3, 3, 5, 2, 2, 7,\n",
       "       1, 1, 1, 2, 2, 0, 9, 5, 7, 9, 2, 2, 5, 2, 4, 3, 1, 1, 8, 2, 1, 1,\n",
       "       4, 9, 7, 8, 5, 9, 6, 7, 3, 1, 9, 0, 3, 1, 3, 5, 4, 5, 7, 7, 4, 7,\n",
       "       9, 4, 2, 3, 8, 0, 1, 6, 1, 1, 4, 1, 8, 3, 9, 6, 6, 1, 8, 5, 2, 9,\n",
       "       9, 8, 1, 7, 7, 0, 0, 6, 9, 1, 2, 2, 9, 2, 6, 6, 1, 9, 5, 0, 4, 7,\n",
       "       6, 7, 1, 8, 1, 1, 2, 8, 1, 3, 3, 6, 2, 4, 9, 9, 5, 4, 3, 6, 7, 4,\n",
       "       6, 8, 5, 5, 4, 3, 1, 8, 4, 7, 6, 0, 9, 5, 1, 3, 8, 2, 7, 5, 3, 4,\n",
       "       1, 5, 7, 0, 4, 7, 5, 5, 1, 0, 9, 6, 9, 0, 8, 7, 8, 8, 2, 5, 2, 3,\n",
       "       5, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1385899 , 0.09366628, 0.09912544, 0.08954167, 0.0953732 ,\n",
       "        0.09269084, 0.08859226, 0.08717955, 0.11331326, 0.10192759],\n",
       "       [0.09133378, 0.10394943, 0.09830302, 0.10459454, 0.10159909,\n",
       "        0.09234006, 0.10484627, 0.10378204, 0.08920828, 0.11004348],\n",
       "       [0.08945984, 0.1002354 , 0.10261187, 0.10875568, 0.10455145,\n",
       "        0.10095053, 0.11206374, 0.10153198, 0.08545591, 0.0943836 ],\n",
       "       [0.11766234, 0.09689345, 0.09988819, 0.09093436, 0.09446678,\n",
       "        0.09145815, 0.09097401, 0.097266  , 0.10728828, 0.11316844]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals.forward(x.reshape(x.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds.argmax(axis = 1) == labels.argmax(axis = 1)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    acc = (predictions.argmax(axis = 1)==targets.argmax(axis = 1)).sum()/predictions.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10['train']._num_examples//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, dataset, loss_module, batch_size = BATCH_SIZE_DEFAULT):\n",
    "    \"\"\"\n",
    "    Test a model for accuracy and loss on a specified dataset. Extended upon the tutorial on Activation Functions.\n",
    "\n",
    "    Inputs:\n",
    "        net -MLP object\n",
    "        dataset - dataset object (train or test)\n",
    "        loss_module - module used to calculate the loss\n",
    "        batch_size - size of fetchted batches\n",
    "    \"\"\"\n",
    "    iters_needed = dataset._num_examples//batch_size # iterations needed to check the whole dataset\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    count = 0\n",
    "    for i in range(iters_needed):\n",
    "        imgs,labels = dataset.next_batch(batch_size)\n",
    "        preds = net.forward(imgs)\n",
    "        accuracies += accuracy(preds, labels)\n",
    "        count += labels.shape[0]\n",
    "        loss += loss_module.forward(preds, labels)\n",
    "    return accuracy, loss \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3, 5])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
