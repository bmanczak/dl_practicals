{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "from dataset import TextDataset\n",
    "from model import TextGenerationModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Using device cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the device which to run the model on\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"[INFO]: Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize dataset with 540241 characters, 87 unique.\n"
     ]
    }
   ],
   "source": [
    "## Parameters \n",
    "batch_size = 64\n",
    "seq_length = 30\n",
    "txt_file = \"assets/book_EN_grimms_fairy_tails.txt\"\n",
    "\n",
    "lstm_num_hidden = 128 #128\n",
    "lstm_num_layers = 2 #2\n",
    "learning_rate = 2e-3\n",
    "max_norm = 10\n",
    "\n",
    "print_every = 100\n",
    "train_steps = 3000\n",
    "embedding_dim = 87\n",
    "\n",
    "dropout_keep_prob = 0\n",
    "num_examples = 5 # how many examples to sample\n",
    "\n",
    "sample_every = 100 #int(train_steps/3) # 3 times during training\n",
    "\n",
    "# Initialize the dataset and data loader (note the +1)\n",
    "dataset = TextDataset(filename=txt_file, seq_length=seq_length)  # fixme\n",
    "data_loader = DataLoader(dataset, batch_size)\n",
    "voc_size = dataset._vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch targets shape: torch.Size([30, 64])\n",
      "Batch inputs shape: torch.Size([30, 64])\n",
      "Batch inputs shape after putting through the embedding: torch.Size([30, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Some experiments to know the dimensions\n",
    "\n",
    "x,y = next(iter(data_loader))\n",
    "#batch_inputs = torch.stack(x).view(seq_length,batch_size, 1).float().to(device) # [batch_size, seq_length, 1], 1 is the input_size\n",
    "batch_inputs = torch.stack(x)\n",
    "batch_targets = torch.stack(y).to(device)\n",
    "\n",
    "print(\"Batch targets shape:\", batch_targets.shape)\n",
    "print(\"Batch inputs shape:\", batch_inputs.shape)\n",
    "\n",
    "embeddings = nn.Embedding(voc_size, embedding_dim)\n",
    "print(\"Batch inputs shape after putting through the embedding:\",embeddings(batch_inputs).shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, seq_length, vocabulary_size,\n",
    "                 lstm_num_hidden=256, lstm_num_layers=2, device='cuda:0', embedding_dim = 32, drop_prob = 0.5, bidirectional = False):\n",
    "\n",
    "        super(TextGenerationModel, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.device = device\n",
    "      \n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = lstm_num_hidden, num_layers = lstm_num_layers, dropout = dropout_keep_prob, bidirectional = bidirectional)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(in_features = lstm_num_hidden, out_features = vocabulary_size)\n",
    "        \n",
    "        h0 = torch.zeros((lstm_num_layers*1,batch_size,lstm_num_hidden))\n",
    "        c0 = torch.zeros((lstm_num_layers*1,batch_size,lstm_num_hidden))\n",
    "        \n",
    "        self.logSoftmax = nn.LogSoftmax(dim=2)\n",
    "        self.embeddings = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x, h = None):\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        if h == None:\n",
    "            lstm_output, h = self.lstm(x) # randomly intialize the hidden state and cell state\n",
    "        else:\n",
    "            lstm_output, h = self.lstm(x, h) # used for sampling from the model\n",
    "            \n",
    "            \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        fc_output = self.fc(drop_output) # shape: [seq_length, batch_size, voc_size]\n",
    "    \n",
    "        return self.logSoftmax(fc_output), h # shape: [seq_length, batch_size, voc_size], a disritbution over the vocabulary\n",
    "    \n",
    "model = TextGenerationModel(batch_size, seq_length, vocabulary_size = dataset._vocab_size, lstm_num_hidden=lstm_num_hidden,\n",
    "                            lstm_num_layers=lstm_num_layers, device = device, embedding_dim= embedding_dim, drop_prob = dropout_keep_prob, bidirectional=bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, dataset, num_examples, example_length):\n",
    "    \"\"\"Uses the model to randomly create batch_size sentences from characters in a dataset\"\"\"\n",
    "    start_letters = [torch.tensor(np.random.randint(0, voc_size)) for i in range(0, num_examples)] # get num_examples random letters\n",
    "    letters = torch.stack(start_letters).view(1,-1) # initial shape [1, num_examples], 0-th dim will increase up to example_lentgh\n",
    "    \n",
    "    h = (torch.zeros((lstm_num_layers*1,num_examples,lstm_num_hidden)), torch.zeros((lstm_num_layers*1,num_examples,lstm_num_hidden)) ) # initialize the hidden state\n",
    "    \n",
    "    next_letter_given_previous = letters # initalize next letter to the random letters\n",
    "    \n",
    "    for letter_num in range(example_length-1):\n",
    "        log_probs, h = model(next_letter_given_previous, h) # store the log probs and the hidden state\n",
    "        \n",
    "\n",
    "        next_letter_given_previous = torch.argmax(log_probs, dim = 2) # get the next most probable letter\n",
    "        letters = torch.cat((letters, next_letter_given_previous), dim=0) # add letters to the generated sequence\n",
    "        \n",
    "        #next_letter_given_previous = torch.argmax(model(letters)[[-1]], dim = 2)\n",
    "        #letters = torch.cat((letters, next_letter_given_previous), dim=0) # add the consecutive letters to the sequence \n",
    "    \n",
    "    return letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-22 15:23] Train Step 0099/2000, Batch Size = 64,                 Examples/Sec = 995.38, Accuracy = 0.56, Loss = 1.450\n",
      "[2020-11-22 15:23] Train Step 0199/2000, Batch Size = 64,                 Examples/Sec = 931.10, Accuracy = 0.51, Loss = 1.624\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-dc4513d1c565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [seq_length, batch_size, voc_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/dl2020/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-634ec5b3e8f1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mlstm_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# randomly intialize the hidden state and cell state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mlstm_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# used for sampling from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/dl2020/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/dl2020/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    577\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "# Setup the loss and optimizer\n",
    "criterion = nn.NLLLoss()  #\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 1e-4 )\n",
    "\n",
    "#param = next(model.parameters()).data\n",
    "#h = (param.new(lstm_num_layers, batch_size, lstm_num_hidden).zero_(), \n",
    "#         param.new(lstm_num_layers, batch_size, lstm_num_hidden).zero_())\n",
    "\n",
    "\n",
    "\n",
    "for step, (batch_inputs, batch_targets) in enumerate(data_loader):\n",
    "\n",
    "    # Only for time measurement of step through network\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Move to GPU\n",
    "    batch_inputs = torch.stack(batch_inputs).to(device) # [batch_size, seq_length]\n",
    "    batch_targets = torch.stack(batch_targets).to(device)  # [batch_size, seq_length]\n",
    "\n",
    "    # Reset for next iteration\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    log_probs, _ = model(batch_inputs) # [seq_length, batch_size, voc_size]\n",
    "    \n",
    "\n",
    "    # Calculate loss, gradients\n",
    "    \"\"\"\n",
    "    loss = 0   #\n",
    "    for timestep in range(seq_length): # iterate over timesteps\n",
    "        loss += criterion(log_probs[timestep, :, :], batch_targets[timestep])\n",
    "    loss /= seq_length\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = criterion(log_probs.view(-1, voc_size), batch_targets.view(-1))\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients to prevent explosion\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                   max_norm=max_norm)\n",
    "\n",
    "    # Update network parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    predictions = torch.argmax(log_probs, dim=2)\n",
    "    correct = (predictions == batch_targets).sum().item()\n",
    "    accuracy = correct/(batch_size*seq_length)\n",
    "    \n",
    "    # Log plots to tensorboard\n",
    "    writer.add_scalar(\"Loss\", loss, step)\n",
    "    writer.add_scalar(\"Accuracy\", accuracy, step)\n",
    "    \n",
    "    # Just for time measurement\n",
    "    t2 = time.time()\n",
    "    examples_per_second = batch_size/float(t2-t1)\n",
    "\n",
    "    \n",
    "    if (step + 1) % print_every == 0:\n",
    "\n",
    "        print(\"[{}] Train Step {:04d}/{:04d}, Batch Size = {}, \\\n",
    "                Examples/Sec = {:.2f}, \"\n",
    "              \"Accuracy = {:.2f}, Loss = {:.3f}\".format(\n",
    "                datetime.now().strftime(\"%Y-%m-%d %H:%M\"), step,\n",
    "                train_steps, batch_size, examples_per_second,\n",
    "                accuracy, loss\n",
    "                ))\n",
    "        \n",
    "    if (step + 1) % sample_every == 0:\n",
    "        \n",
    "    if step == train_steps:\n",
    "        break\n",
    "        \n",
    "writer.flush()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextGenerationModel(\n",
       "  (lstm): LSTM(87, 128, num_layers=2)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=87, bias=True)\n",
       "  (logSoftmax): LogSoftmax(dim=2)\n",
       "  (embeddings): Embedding(87, 87)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, dataset, num_examples, example_length):\n",
    "    \"\"\"Uses the model to randomly create batch_size sentences from characters in a dataset\"\"\"\n",
    "    start_letters = [torch.tensor(np.random.randint(0, voc_size)) for i in range(0, num_examples)] # get num_examples random letters\n",
    "    letters = torch.stack(start_letters).view(1,-1) # initial shape [1, num_examples], 0-th dim will increase up to example_lentgh\n",
    "    \n",
    "    h = (torch.zeros((lstm_num_layers*1,num_examples,lstm_num_hidden)), torch.zeros((lstm_num_layers*1,num_examples,lstm_num_hidden)) ) # initialize the hidden state\n",
    "    \n",
    "    next_letter_given_previous = letters # initalize next letter to the random letters\n",
    "    \n",
    "    for letter_num in range(example_length-1):\n",
    "        log_probs, h = model(next_letter_given_previous, h) # store the log probs and the hidden state\n",
    "        \n",
    "\n",
    "        next_letter_given_previous = torch.argmax(log_probs, dim = 2) # get the next most probable letter\n",
    "        letters = torch.cat((letters, next_letter_given_previous), dim=0) # add letters to the generated sequence\n",
    "        \n",
    "        #next_letter_given_previous = torch.argmax(model(letters)[[-1]], dim = 2)\n",
    "        #letters = torch.cat((letters, next_letter_given_previous), dim=0) # add the consecutive letters to the sequence \n",
    "    \n",
    "    return letters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentences = generate_text(model, dataset, 3, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E â€˜The wind the world to the world to the world to'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.convert_to_string([letter.item() for letter in example_sentences[:,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = np.random.randint(0, len(dataset._data)-dataset._seq_length-2)\n",
    "inputs = [dataset._char_to_ix[ch] for ch in dataset._data[offset:offset+dataset._seq_length]]\n",
    "offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_letters = [torch.tensor(np.random.randint(0, voc_size)) for i in range(0, 5)]\n",
    "random_letters = torch.stack(start_letters).view(1,-1)\n",
    "\n",
    "second_letters = torch.argmax(model(random_letters), dim = 2)\n",
    "letters = torch.cat([random_letters, second_letters])\n",
    "\n",
    "model(letters)[[-1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = vectorizer.surname_vocab.start_index\n",
    "batch_size_new = 2\n",
    "# hidden_size = whatever hidden size the model is set to\n",
    "\n",
    "initial_h = Variable(torch.ones(batch_size, hidden_size))\n",
    "initial_x_index = Variable(torch.ones(batch_size).long()) * start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = torch.tensor([[1],\n",
    "                       [2], \n",
    "                       [3]]).T\n",
    "example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(example).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(list(dataset._ix_to_char.keys()))\n",
    "model.embeddings(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_next_char(model, char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
